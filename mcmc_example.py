import corner
import emcee
import math
import matplotlib.pyplot as plt
import numpy as np







###########################

# The data

# Note: 
# 	The data was generated by adding noise to a Gaussian with centre = 0, height = 1, sigma = 1

###########################

x_axis 			= [-5.0, -4.9, -4.8, -4.7, -4.6, -4.5, -4.4, -4.3, -4.2, -4.1, -4.0, -3.9, -3.8, -3.7, -3.6, -3.5, -3.4, -3.3, -3.2, -3.1, -3.0, -2.9, -2.8, -2.7, -2.6, -2.5, -2.4, -2.3, -2.2, -2.1, -2.0, -1.9, -1.8, -1.7, -1.6, -1.5, -1.4, -1.3, -1.2, -1.1, -1.0, -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0, 4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9, 5.0]
spectrum 		= [0.0793, -0.1485, -0.1313, 0.1168, 0.0692, -0.0021, 0.1479, -0.0066, 0.1279, 0.1103, 0.0703, 0.0323, -0.0009, 0.0600, -0.0138, 0.0995, -0.0101, -0.1405, -0.0015, -0.0394, -0.1294, 0.1394, -0.0136, 0.0358, 0.1519, 0.0669, -0.0202, -0.0731, 0.2034, 0.1811, 0.1070, 0.2872, 0.1640, 0.3299, 0.3872, 0.2129, 0.2924, 0.3831, 0.5188, 0.5407, 0.7137, 0.7388, 0.7094, 0.8603, 0.9573, 0.9627, 0.9936, 1.0240, 0.9076, 0.9784, 1.1380, 1.0535, 0.8761, 0.8974, 0.9425, 0.8655, 0.7386, 0.6381, 0.8499, 0.5404, 0.5643, 0.4140, 0.5495, 0.5727, 0.3065, 0.3210, 0.1952, 0.2053, 0.2853, 0.2644, 0.1033, 0.1298, 0.0597, 0.0164, 0.0291, -0.0244, -0.0446, 0.0276, 0.0053, -0.1169, 0.1583, -0.0957, -0.0123, 0.0738, 0.1016, 0.1518, -0.0406, -0.0104, 0.0381, 0.1214, 0.0548, 0.0193, -0.0513, 0.1378, 0.1229, -0.0850, -0.1013, 0.1372, 0.0516, -0.0695, -0.0599]
spectrum_rms 	= 0.3







###########################

# Plot the data

###########################

plt.figure()
plt.plot(x_axis, spectrum, color = 'black', label = 'Data')
plt.legend(loc = 0)
plt.show()
plt.close()







###########################

# The data has a roughly Gaussian shape, so let's fit a Gaussian to it.
# Form of a Gaussian:
#	f(x) = height * e^((x - mean)^2 / (2 * sigma))

###########################







###########################

# Functions

###########################

def Gaussian(mean = None, fwhm = None, height = None, sigma = None, amp = None):
	'''
	Generates a Gaussian profile with the given parameters.
	'''

	if sigma == None:
		sigma = fwhm / (2. * math.sqrt(2. * np.log(2.)))

	if height == None:
		height = amp / (sigma * math.sqrt(2.* math.pi))

	return lambda x: height * np.exp(-((x - mean)**2.) / (2.*sigma**2.))
def LogPrior(x, x_axis, spectrum, rms):
	'''
	Calculates the natural log of the prior distribution at a position in parameter space defined by the vector x.
		- Output values range from negative infinity (Pr = 0) to 0 (Pr = 1)
		- The integral of the prior distribution over parameter space must equal 1: the distributions must be normalised to ensure this.

	x = position in parameter space: [centroid, sigma, height]
	x_axis = set of x values for the data points
	spectrum = set of y-axis values for the data points
	rms = rms of spectrum
	'''

	log_prior = 1.
	[centroid, sigma, height] = x
	
	# These are naive priors, we only want to limit parameter space
	# centroid
	x_min = min(x_axis)
	x_max = max(x_axis)
	if centroid <= x_max and centroid >= x_min:
		log_prior += np.log(1/(x_max - x_min)) # thus the prior is normalised to integrate to 1 over parameter space
	else:
		return -np.inf

	# sigma
	if sigma > 0. and sigma < 10.:
		log_prior += np.log(1/10.) # thus the prior is normalised to integrate to 1 over parameter space
	else:
		return -np.inf

	# height
	min_spectrum = min(spectrum)
	max_spectrum = max(spectrum)
	if height <= 5. * abs(max_spectrum) and height >= -5. * abs(min_spectrum): # seems verbose but eliminates the edge case where min is positive
		log_prior += np.log(1/(5. * abs(max_spectrum) + 5. * abs(min_spectrum)))
	else:
		return -np.inf

	return log_prior
def LogLikelihood(x, x_axis, spectrum, rms):
	'''
	Calculates the natural log of the likelihood at the position 'x' in parameter space and adds this to the natural log of the prior.
		- The integral of the likelihood distribution over data space must equal 1: the distribution must be normalised to ensure this.

	x = position in parameter space: [centroid, sigma, height]
	x_axis = set of x values for the data points
	spectrum = set of y-axis values for the data points
	rms = rms of spectrum
	'''
	log_prior = LogPrior(x, x_axis, spectrum, rms)

	[centroid, sigma, height] = x

	if log_prior != -np.inf:
		model = Gaussian(mean = centroid, sigma = sigma, height = height)(x_axis)
		N = len(spectrum)
		log_likelihood = ((-sum((spectrum - model)**2)) / (2. * rms**2)) - (N * np.log(math.sqrt(2 * math.pi) * rms)) # This is derived from 
		return log_likelihood + log_prior
	else:
		return -np.inf







###########################

# Set parameters for emcee sampler object

###########################

num_walkers = 20 # needs to be more then twice the number of dimensions. 100 is way too many
num_dim = 3 # number of dimensions of the model (Gaussians have 3)
burn_iterations = 100 # initial phase
final_iterations = 100 # exploration phase







###########################

# Initial positions in parameter space for each walker.
# 	- These are random but they can be found from a faster fitting algorithm or be some kind of informed guess.

###########################

p0 = [[np.random.uniform(-3, 3), np.random.uniform(0.1, 2), np.random.uniform(2. * min(spectrum), 2. * max(spectrum))] for x in range(num_walkers)]







###########################

# Initialise sampler object

###########################

sampler = emcee.EnsembleSampler(num_walkers, num_dim, LogLikelihood, args = [x_axis, spectrum, spectrum_rms])







###########################

# Run emcee: 'Burn-in' phase
#	- This step allows the walkers to locate the best region of parameter space.
#	- In the next step we'll derive statistics from where the walkers visit, so 
#		we need them to already be in the right place.

###########################

pos, prob, state = sampler.run_mcmc(p0, burn_iterations)







###########################

# Plot the walkers in the 'Burn-in' phase.
#	- You should be able to see that the walkers have 'found' an area in parameter space that they like

###########################

fig, axes = plt.subplots(nrows = 3, ncols = 1, sharex = True)

axes[0].set_title('Burn-in Phase')
for walker in range(sampler.chain.shape[0]):
	axes[0].set_ylabel('Centroid')
	axes[0].plot(range(sampler.chain.shape[1]), sampler.chain[walker,:,0])
for walker in range(sampler.chain.shape[0]):
	axes[1].set_ylabel('sigma')
	axes[1].plot(range(sampler.chain.shape[1]), sampler.chain[walker,:,1])
for walker in range(sampler.chain.shape[0]):
	axes[2].set_ylabel('sigma')
	axes[2].plot(range(sampler.chain.shape[1]), sampler.chain[walker,:,2])
plt.xlabel('Iterations')
plt.show()
plt.close()







###########################

# Run emcee: 'Final' phase
#	- This step allows the walkers to explore the best region of parameter space.

###########################

sampler.reset()
sampler.run_mcmc(pos, final_iterations)







###########################

# Plot the walkers in the 'Final' phase.

###########################

fig, axes = plt.subplots(nrows = 3, ncols = 1, sharex = True)

axes[0].set_title('Final Phase')
for walker in range(sampler.chain.shape[0]):
	axes[0].set_ylabel('Centroid')
	axes[0].plot(range(sampler.chain.shape[1]), sampler.chain[walker,:,0])
for walker in range(sampler.chain.shape[0]):
	axes[1].set_ylabel('sigma')
	axes[1].plot(range(sampler.chain.shape[1]), sampler.chain[walker,:,1])
for walker in range(sampler.chain.shape[0]):
	axes[2].set_ylabel('sigma')
	axes[2].plot(range(sampler.chain.shape[1]), sampler.chain[walker,:,2])
plt.xlabel('Iterations')
plt.show()
plt.close()







###########################

# Corner plots help diagnose issues with correlations between parameters

###########################

figure = corner.corner(np.transpose([sampler.flatchain[:,0], sampler.flatchain[:,1], sampler.flatchain[:,2]]), show_titles = True, range = (0.9, 0.9, 0.9), labels = ['Centroid', 'Sigma', 'Height'])
plt.show()
plt.close()







###########################

# Find 'best' parameters
#	- The median position of converged walkers is the easiest way to choose the 'best' parameters.
#	- The +/- 1 sigma uncertainties are then the 16th and 84th quantiles

###########################

centroid_fit 	= corner.quantile(sampler.flatchain[:,0], [0.16, 0.50, 0.84])
sigma_fit 		= corner.quantile(sampler.flatchain[:,1], [0.16, 0.50, 0.84])
height_fit 		= corner.quantile(sampler.flatchain[:,2], [0.16, 0.50, 0.84])
model_fit 		= Gaussian(mean = centroid_fit[1], sigma = sigma_fit[1], height = height_fit[1])(x_axis)







###########################

# Report best parameters

###########################

print 'Fitted Parameters:'
print '\tCentroid = ' + str(centroid_fit[1]) + ' +/- ' + str(abs((centroid_fit[2] - centroid_fit[0]) / 2))
print '\tSigma = ' + str(sigma_fit[1]) + ' +/- ' + str(abs((sigma_fit[2] - sigma_fit[0]) / 2))
print '\tHeight = ' + str(height_fit[1]) + ' +/- ' + str(abs((height_fit[2] - height_fit[0]) / 2))







###########################

# Plot model

###########################

plt.figure()
plt.plot(x_axis, spectrum, color = '0.5', label = 'Data')
plt.plot(x_axis, model_fit, color = 'red', label = 'Fit')
plt.legend(loc = 0)
plt.show()
plt.close()

